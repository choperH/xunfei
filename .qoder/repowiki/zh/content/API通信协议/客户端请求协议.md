# 客户端请求协议

<cite>
**本文档引用的文件**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py)
</cite>

## 目录
1. [引言](#引言)  
2. [WebSocket消息格式概述](#websocket消息格式概述)  
3. [首帧消息（status=0）](#首帧消息status0)  
4. [中间帧消息（status=1）](#中间帧消息status1)  
5. [末帧消息（status=2）](#末帧消息status2)  
6. [Ws_Param类与iat_params参数详解](#ws_param类与iat_params参数详解)  
7. [音频分帧与发送控制机制](#音频分帧与发送控制机制)  
8. [实际JSON消息示例](#实际json消息示例)  
9. [常见配置错误与调试建议](#常见配置错误与调试建议)

## 引言
本文档详细描述了客户端向讯飞语音识别服务发送的WebSocket消息格式。该协议采用流式传输方式，将音频数据分帧发送，并通过状态码区分不同阶段的数据帧。整个过程分为三个阶段：首帧、中间帧和末帧，分别对应`status=0`、`status=1`和`status=2`。文档将结合代码实现，深入解析每种帧的结构、参数含义及发送逻辑。

## WebSocket消息格式概述
客户端通过WebSocket连接讯飞语音识别服务时，需按照特定格式构造JSON消息体。消息结构统一包含三个部分：`header`、`parameter`和`payload`。其中：
- `header`：包含请求状态和应用标识；
- `parameter`：携带识别参数，如语言、领域等；
- `payload`：封装音频数据及其元信息。

根据音频流的发送阶段，`header`中的`status`字段取值不同，用于标识当前帧的类型。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L130-L197)

## 首帧消息（status=0）
首帧是音频流的起始帧，必须在连接建立后第一时间发送。其JSON结构如下：

```json
{
  "header": {
    "status": 0,
    "app_id": "开发者APPID"
  },
  "parameter": {
    "iat": {
      "domain": "slm",
      "language": "zh_cn",
      "accent": "mandarin",
      "dwa": "wpgs",
      "result": {
        "encoding": "utf8",
        "compress": "raw",
        "format": "plain"
      }
    }
  },
  "payload": {
    "audio": {
      "audio": "Base64编码的音频数据",
      "sample_rate": 16000,
      "encoding": "raw"
    }
  }
}
```

### 字段说明
- `header.status`：固定为`0`，表示首帧；
- `header.app_id`：开发者在讯飞开放平台注册的应用ID；
- `parameter.iat`：语音识别参数集合；
- `payload.audio.audio`：第一帧的音频数据，以Base64编码；
- `sample_rate`：采样率为16000Hz；
- `encoding`：原始音频格式（raw）。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L150-L159)

## 中间帧消息（status=1）
中间帧用于持续传输音频数据，结构与首帧基本一致，仅`header.status`改为`1`。

```json
{
  "header": {
    "status": 1,
    "app_id": "开发者APPID"
  },
  "parameter": {
    "iat": { ... }
  },
  "payload": {
    "audio": {
      "audio": "Base64编码的音频数据",
      "sample_rate": 16000,
      "encoding": "raw"
    }
  }
}
```

> **注意**：尽管`parameter`字段在每次发送时都包含，但其内容在整个会话中保持不变。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L165-L173)

## 末帧消息（status=2）
末帧表示音频数据已全部发送完毕，触发服务端开始最终识别并关闭连接。

```json
{
  "header": {
    "status": 2,
    "app_id": "开发者APPID"
  },
  "parameter": {
    "iat": { ... }
  },
  "payload": {
    "audio": {
      "audio": "",
      "sample_rate": 16000,
      "encoding": "raw"
    }
  }
}
```

### 特点
- `status`设为`2`；
- `payload.audio.audio`可为空字符串；
- 发送后立即终止发送循环。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L179-L188)

## Ws_Param类与iat_params参数详解
`Ws_Param`类用于封装请求所需的所有参数，其中`iat_params`字段定义了语音识别的核心配置。

### iat_params配置项
```python
self.iat_params = {
    "domain": "slm",           # 领域：slm表示大模型领域
    "language": "zh_cn",       # 语言：中文普通话
    "accent": "mandarin",      # 口音：标准普通话
    "dwa": "wpgs",             # 启用动态修正功能
    "result": {
        "encoding": "utf8",
        "compress": "raw",
        "format": "plain"
    }
}
```

### 参数含义
- `domain=slm`：使用大语言模型进行识别，提升语义理解能力；
- `dwa=wpgs`：启用“动态词权重调整”（Write-back with Partial Grammar Support），支持实时热词修正；
- `language`和`accent`：指定识别语言和口音，确保准确匹配中文普通话。

这些参数在每次消息发送时都会被嵌入到`parameter`字段中。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L51-L58)

## 音频分帧与发送控制机制
`on_open`回调函数负责读取音频文件、分帧并按指定间隔发送。

### 分帧逻辑
- 每帧大小：`1280`字节（约40ms的16kHz单声道PCM数据）；
- 读取方式：使用`fp.read(frameSize)`逐帧读取二进制数据；
- 编码方式：使用`base64.b64encode`对音频数据进行编码。

### 发送间隔控制
- 间隔时间：`intervel = 0.04`秒（即40ms）；
- 控制方式：通过`time.sleep(intervel)`模拟真实录音流的节奏；
- 目的：避免发送过快导致服务端处理压力过大或丢包。

### 帧状态判断
- 初始状态为`STATUS_FIRST_FRAME`（0）；
- 文件读取结束后，将状态置为`STATUS_LAST_FRAME`（2）；
- 其余情况为`STATUS_CONTINUE_FRAME`（1）。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L131-L194)

## 实际JSON消息示例
以下为一个完整的首帧消息示例：

```json
{
  "header": {
    "status": 0,
    "app_id": "your_app_id_12345"
  },
  "parameter": {
    "iat": {
      "domain": "slm",
      "language": "zh_cn",
      "accent": "mandarin",
      "dwa": "wpgs",
      "result": {
        "encoding": "utf8",
        "compress": "raw",
        "format": "plain"
      }
    }
  },
  "payload": {
    "audio": {
      "audio": "UklGRiQAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQAAAAgAAACQjwAA",
      "sample_rate": 16000,
      "encoding": "raw"
    }
  }
}
```

中间帧和末帧仅需更改`header.status`值即可。

## 常见配置错误与调试建议
### 常见错误
1. **APPID或密钥未填写**：在`Ws_Param`初始化时未提供有效`APPID`、`APIKey`或`APISecret`；
2. **音频格式不支持**：非16kHz、16位、单声道PCM格式；
3. **编码错误**：未对音频数据进行Base64编码；
4. **发送频率不当**：间隔过短导致服务端拒绝或过长影响实时性；
5. **未正确设置末帧**：未发送`status=2`导致服务端无法结束识别。

### 调试建议
- 启用`websocket.enableTrace(True)`查看底层通信日志；
- 检查`create_url()`生成的URL是否包含正确的鉴权参数；
- 使用小文件测试流程完整性；
- 对比官方文档中的[错误码](https://www.xfyun.cn/document/error-code)排查`code != 0`的情况；
- 确保音频文件路径正确且可读。

**Section sources**  
- [大模型中文语音识别.py](file://大模型中文语音识别.py#L203-L211)